The results illustrate the typical differences between on-policy (SARSA) and off-policy (Q-Learning) learning algorithms. SARSA tends to be more stable, especially noticeable in the smoother curve after the initial learning phase, because it updates its Q-values based on the action actually taken, leading to more conservative learning. In contrast, Q-Learning evaluates the optimal action (the one with the highest estimated Q-value), which can lead to faster learning but at the cost of higher variability. Both algorithms, after sufficient episodes, converge to similar average rewards, demonstrating their ability to learn effective navigation strategies in the CliffWalking environment.